{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c69becf",
   "metadata": {},
   "source": [
    "# Beating Brock in Pokémon Red using Reinforcement Learning\n",
    "\n",
    "## By: Patrick Sharp\n",
    "\n",
    "It's recommend you read this notebook from the [GitHub](https://github.com/PSharp725/Pokemon-Red-RL/blob/main/src/notebooks/report.ipynb) to be able to see the embedded videos."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42866464",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project aims to explore Reinforcement Learning (RL) algorithms within the context of classic video game environments, specifically focusing on Pokémon Red. The project's goals are twofold:\n",
    "\n",
    "1. to implement and evaluate various RL algorithms in a custom Gymnasium environment based on Pokémon Red (thanks to [this repo](https://github.com/PWhiddy/PokemonRedExperiments/tree/master) by PWhiddy), and\n",
    "\n",
    "2. to investigate the critical balance between exploration and exploitation in RL agent performance.\n",
    "\n",
    "Unlike many traditional RL projects that focus on low-dimensional or heavily simplified environments (e.g., CartPole, FrozenLake), this project attempts to tackle a partially observable, high-variance game world. This naturally introduces complexity in terms of state representation, reward sparsity, and policy generalization.\n",
    "\n",
    "The ultimate practical goal is to train agents capable of defeating Brock, the first gym leader, thereby obtaining the Boulder Badge — a milestone early in the game but nontrivial in terms of action selection, state abstraction, and long-term planning.\n",
    "\n",
    "### What is Pokémon Red?\n",
    "\n",
    "Pokémon Red (1996, Game Freak/Nintendo) is a role-playing video game (RPG) in which players control a protagonist navigating a fictional world, capturing and training creatures called Pokémon, and battling other trainers to earn badges and progress the storyline.\n",
    "\n",
    "From an RL perspective, Pokémon Red presents a sequential decision-making problem with attributes including:\n",
    "\n",
    "- Partial Observability: The agent cannot directly observe true world states (e.g., enemy Pokémon's hidden stats).\n",
    "\n",
    "- Long-Term Dependencies: Success depends not just on immediate actions but on strategies developed across long sequences (e.g., choosing to train a Pokémon early affects performance hours later).\n",
    "\n",
    "- Stochasticity: Many events (critical hits, enemy move choices) introduce randomness into outcomes.\n",
    "\n",
    "- Sparse Rewards: Winning a battle or earning a badge occurs only after potentially hundreds of intermediate steps without explicit reward signals.\n",
    "\n",
    "Thus, it provides a rich testbed beyond simplistic RL benchmarks.\n",
    "\n",
    "### Why Pokémon Red?\n",
    "\n",
    "Pokémon Red was selected due to personal nostalgia, as it was my first introduction to video games at the age of four, played on my yellow Gameboy Pocket. This nostalgic connection provides intrinsic motivation to dive deeper into the problem space.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Setting up the environment \n",
    "\n",
    "### PyBoy\n",
    "\n",
    "[PyBoy](https://github.com/Baekalfen/PyBoy) is a Python-based emulator for the Nintendo Game Boy, designed to provide programmatic access to the emulation process through a clean API. It allows external scripts to read game memory, send controller inputs, and observe screen outputs — all crucial capabilities for integrating reinforcement learning agents with a game environment that was never originally designed for AI training.\n",
    "\n",
    "For this project, PyBoy acts as the critical bridge between the RL algorithms and Pokémon Red. It enables the custom Gymnasium environment to interface directly with the game's internal state, sending actions (e.g., pressing 'A', 'Start', navigating menus) and receiving observations (e.g., screen pixels, memory values) in a way that is compatible with modern RL pipelines. Without such programmatic control and visibility into game state, training agents in a complex environment like Pokémon Red would be effectively infeasible.\n",
    "\n",
    "Moreover, using PyBoy ensures deterministic, reproducible experiments — an essential property for debugging RL agents, evaluating exploration strategies, and properly measuring algorithmic performance.\n",
    "\n",
    "We will be using PyBoy to help with running our environment.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The game environment takes these controls and creates the following action lists that can be used within the environment wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ea71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using SDL2 binaries from pysdl2-dll*\")\n",
    "from pyboy.utils import WindowEvent\n",
    "\n",
    "valid_actions = [\n",
    "            WindowEvent.PRESS_ARROW_DOWN,\n",
    "            WindowEvent.PRESS_ARROW_LEFT,\n",
    "            WindowEvent.PRESS_ARROW_RIGHT,\n",
    "            WindowEvent.PRESS_ARROW_UP,\n",
    "            WindowEvent.PRESS_BUTTON_A,\n",
    "            WindowEvent.PRESS_BUTTON_B,\n",
    "            WindowEvent.PRESS_BUTTON_START,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953847c7",
   "metadata": {},
   "source": [
    "\n",
    "![alt text](../assets/images/Pokemon_red_controls.png \"Controls\")\n",
    "\n",
    "Nintendo. (1996) Pokémon Red Trainer's Guide. Nintendo of America Inc. Retrieved from https://pokemon-project.com/juegos/manual/manual-GB-Pokemon-Rojo-Azul-EN.pdf\n",
    "\n",
    "---\n",
    "\n",
    "### Verifying the game file\n",
    "\n",
    "Before using Pokémon Red within the custom Gymnasium environment, it is critical to ensure that the game file (ROM) being used matches the expected version supported by the environment. To do this, we verify the integrity of the PokemonRed.gb file by calculating its SHA-1 checksum.\n",
    "\n",
    "Using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5ee7d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ea9bcae617fdf159b045185467ae58b2e4a48b9a  ../../PokemonRed.gb\n"
     ]
    }
   ],
   "source": [
    "# Check the hash of the ROM file\n",
    "!shasum ../../PokemonRed.gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00529968",
   "metadata": {},
   "source": [
    "we compute the SHA-1 hash of the ROM file. The expected hash, according to PWhiddy's repository documentation, is: `ea9bcae617fdf159b045185467ae58b2e4a48b9a`.\n",
    "\n",
    "If the output of the command matches this expected value, it confirms that the ROM file is identical at the binary level to the one the custom Gymnasium environment was built and tested against."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb3f5d9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The Environment\n",
    "\n",
    "The environment can determine the following keys for the gamestate:\n",
    "\n",
    "| Observation Key | Description |Type |\n",
    "|------------|-------------|---------------|\n",
    "| event      | Number of events observed | int |\n",
    "| level      | Sum of all Pokémon levels | int |\n",
    "| heal       | Amount of healing from items or visiting a Pokémon Center | float |\n",
    "| op_lvl     | Opponent Pokémon's level | int |\n",
    "| dead       | Number of times \"dying\" (Pokémon fainting) | int |\n",
    "| badge      | Number of Gym Badges | int |\n",
    "| explore    | Number of map tiles visited | int |\n",
    "| stuck      | Number of times stuck | int |\n",
    "\n",
    "\n",
    "The reward structure is designed to encourage both exploration and meaningful game progress. Events triggered and new tiles visited serve to promote thorough map exploration and storyline advancement. Achieving Gym Badges and capturing Pokémon are explicitly incentivized, with further rewards tied to strengthening the player's team through leveling. To foster strategic and sustainable gameplay, the agent is rewarded for maintaining the health of its Pokémon through healing, while deaths are penalized. Additionally, the system detects and penalizes situations where the agent becomes stuck, encouraging continuous forward movement and discouraging inefficient behaviors.\n",
    "\n",
    "An example of the environment setup can be seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ba0394",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from red_gym_env import RedGymEnv\n",
    "\n",
    "STATE_REWARDS = {\n",
    "    \"event\": True,\n",
    "    \"level\": False,\n",
    "    \"heal\": True,\n",
    "    \"op_lvl\": False,\n",
    "    \"dead\": False,\n",
    "    \"badge\": True,\n",
    "    \"explore\": True,\n",
    "    \"stuck\": True,\n",
    "}\n",
    "STATE_REWARD_WEIGHTS = {\n",
    "    \"event\": 4,\n",
    "    \"level\": 1,\n",
    "    \"heal\": 10,\n",
    "    \"op_lvl\": 0.2,\n",
    "    \"dead\": -0.1,\n",
    "    \"badge\": 10,\n",
    "    \"explore\": 0.1,\n",
    "    \"stuck\": -0.05,\n",
    "}\n",
    "REWARD_SCALE = 0.5\n",
    "EXPLORE_WEIGHT = 0.25\n",
    "EP_LENGTH = 2048 * 80\n",
    "NUM_CPU = 32  # Also sets the number of episodes per training iteration\n",
    "\n",
    "env_config = {\n",
    "    'headless': True,\n",
    "    'save_final_state': True,\n",
    "    'early_stop': False,\n",
    "    'action_freq': 24,\n",
    "    'init_state': './init.state',\n",
    "    'max_steps': EP_LENGTH,\n",
    "    'save_video': True,\n",
    "    'fast_video': False,\n",
    "    'session_path': 'temp_path',\n",
    "    'gb_path': './PokemonRed.gb',\n",
    "    'debug': False,\n",
    "    'reward_scale': REWARD_SCALE,\n",
    "    'explore_weight': EXPLORE_WEIGHT,\n",
    "    'print_rewards': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4123ef1b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Approach\n",
    "\n",
    "### Random Agent\n",
    "\n",
    "To establish a performance baseline, a RandomAgent was implemented. The agent is a subclass of a generic BaseAgent, and its decision policy is trivially simple: it samples an action uniformly at random from the environment's available action_space. The relevant code structure is as follows:\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "from agents.base_agent import BaseAgent\n",
    "\n",
    "class RandomAgent(BaseAgent):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__(action_space)\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "env = RedGymEnv(env_config)\n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    " while not done:\n",
    "    action = agent.select_action(obs)\n",
    "    obs, reward, _, done, _ = env.step(action)\n",
    "    env.save_and_print_info(done, obs)\n",
    "```\n",
    "\n",
    "At each timestep, the agent selects a random action without considering the environment's current state (observation), any past experience, or the long-term consequences of its actions. This leads to purely stochastic behavior, serving as a control for comparison against more sophisticated reinforcement learning algorithms.\n",
    "\n",
    "The agent was tested in a custom Gymnasium environment that simulates Pokémon Red gameplay (using a RedGym environment wrapper). The agent was allowed to operate from the initial game start until a termination condition was reached (Episode length).\n",
    "\n",
    "#### Gameplay Behavior and Observations\n",
    "\n",
    "An analysis of the gameplay rollout shows the RandomAgent displaying several key characteristics:\n",
    "\n",
    "- Erratic and Inefficient Movement: The agent frequently alternates between movement directions without consistent navigation goals. For example, it may move up briefly, then left, then up again, then open menus without any strategic pattern.\n",
    "\n",
    "- Excessive Menu Interaction: Because random button presses include selecting the \"Start\" button, the agent often opens the game menu inadvertently. Upon opening the menu, it issues random inputs, sometimes moving the cursor but rarely exiting the menu intentionally. This behavior significantly interrupts any forward gameplay progress.\n",
    "\n",
    "- Minimal Progress Toward Objectives: The agent fails to make meaningful progress toward defeating the first Gym Leader, Brock, or even toward reaching the Viridian City Pokémon Center. Any movement toward objectives is purely coincidental and almost immediately undone by subsequent random actions.\n",
    "\n",
    "- High Frequency of No-Op Actions: Many random inputs have little to no effect on the environment's state (e.g., pressing a directional input when movement is blocked by a wall). These actions contribute to wasted timesteps and further delay progress.\n",
    "\n",
    "Critical Evaluation\n",
    "\n",
    "The gameplay of the random agent highlights the extreme inefficiency of unguided exploration in large, structured environments like Pokémon Red. Even with a simple spatial goal such as reaching a nearby city, random movement fails spectacularly because:\n",
    "\n",
    "- State-Space Size: Pokémon Red has a very large and complex state space. Random actions almost never produce beneficial state transitions.\n",
    "\n",
    "- Sparse Rewards: Positive feedback (e.g., gaining experience, winning battles) is sparse and conditional on complex sequences of actions. Random agents are exceedingly unlikely to stumble into these sequences by chance.\n",
    "\n",
    "- Structured Tasks: The game requires highly structured sequences (e.g., talking to an NPC, navigating menus carefully) that random behavior simply cannot achieve.\n",
    "\n",
    "As a result, the RandomAgent acts as a clear lower bound on performance. Future agents must significantly outperform this baseline to be considered successful.\n",
    "\n",
    "<img src=\"../assets/gifs/Random_agent.gif\" alt=\"Random Agent\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d5f171",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## PPO\n",
    "\n",
    "### Agent Selection Rationale\n",
    "\n",
    "For the first reinforcement learning baseline beyond random behavior, the **Proximal Policy Optimization (PPO)** algorithm was selected.  \n",
    "PPO is a policy-gradient method that strikes a strong balance between **training stability** and **sample efficiency**, two factors critical in complex environments like Pokémon Red.\n",
    "\n",
    "**Key reasons for choosing PPO include:**\n",
    "- **Robustness**: PPO is known for its reliability and ease of tuning compared to older policy-gradient methods like Vanilla Policy Gradient (VPG) or Trust Region Policy Optimization (TRPO) ([Schulman et al., 2017](https://arxiv.org/abs/1707.06347)).\n",
    "- **Efficient Use of Data**: PPO uses a clipped objective function that avoids making overly large policy updates, allowing for more stable learning from batches of collected experience.\n",
    "- **Good for High-Dimensional Action Spaces**: Pokémon Red's environment involves both spatial navigation and complex interaction mechanics (menus, items, NPC dialogues), making PPO's generality across discrete and continuous actions highly advantageous.\n",
    "- **Wide Empirical Success**: PPO has been successfully applied to a wide range of tasks from robotics control to game playing, making it a safe and widely trusted starting point for early agent development.\n",
    "\n",
    "Thus, PPO serves as a **natural first choice** for building an agent capable of strategic gameplay in Pokémon Red.\n",
    "\n",
    "\n",
    "\n",
    "### PPO Agent Implementation Details\n",
    "\n",
    "The PPO agent was implemented using [**Stable-Baselines3**](https://github.com/DLR-RM/stable-baselines3)'s `PPO` algorithm, with some modifications to better handle the complexity of Pokémon Red gameplay.\n",
    "\n",
    "To improve sample efficiency and training speed, a vectorized environment setup (`SubprocVecEnv`) was used to run multiple instances of the RedGym environment in parallel. Each parallel environment was initialized with a unique random seed to encourage diverse experiences across subprocesses.\n",
    "\n",
    "The core environment creation function was structured as follows:\n",
    "\n",
    "```Python\n",
    "def make_env(rank, env_conf, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the initial seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = RedGymEnv(env_conf)\n",
    "        env.reset(seed=(seed + rank))\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "\n",
    "num_cpu = NUM_CPU \n",
    "env = SubprocVecEnv([make_env(i, env_config) for i in range(NUM_CPU)])\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=ep_length//2,\n",
    "    save_path=sess_path,\n",
    "    name_prefix=\"poke\"\n",
    ")   \n",
    "callbacks = [checkpoint_callback, TensorboardCallback(sess_path)]\n",
    "\n",
    "model = PPO(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    n_steps=train_steps_batch,\n",
    "    batch_size=512,\n",
    "    n_epochs=1,\n",
    "    gamma=0.997,\n",
    "    ent_coef=0.01,\n",
    "    tensorboard_log=sess_path\n",
    ")\n",
    "model.learn(\n",
    "    total_timesteps=(ep_length) * num_cpu * 10_000,  # Attempt to run 10,000 iterations\n",
    "    callback=CallbackList(callbacks),\n",
    "    tb_log_name=\"poke_ppo\"\n",
    ")\n",
    "```\n",
    "\n",
    "\n",
    "#### Overview of PPO\n",
    "\n",
    "Proximal Policy Optimization (PPO) is a policy-gradient reinforcement learning algorithm that optimizes the expected cumulative reward by adjusting the agent’s action policy directly.  \n",
    "Unlike traditional policy gradients, which can suffer from instability due to large updates, PPO introduces a clipped objective that penalizes updates which move too far from the previous policy.\n",
    "\n",
    "This ensures that learning proceeds in small, trusted steps, maintaining a balance between improving the policy and preserving exploration.\n",
    "\n",
    "Formally, the core PPO objective is:\n",
    "\n",
    "\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E} \\left[ \\min \\left( r_t(\\theta) \\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) \\hat{A}_t \\right) \\right]$$\n",
    "\n",
    "\n",
    "where $r_t(\\theta)$ is the probability ratio between the new and old policy, and  $\\hat{A}_t$ is the advantage function estimating the relative value of an action compared to baseline.\n",
    "\n",
    "\n",
    "\n",
    "#### Gameplay Behavior and Observations\n",
    "\n",
    "Reviewing the PPO agent’s rollout shows clear signs of learning compared to the RandomAgent baseline:\n",
    "\n",
    "- **Purposeful Movement**: The agent demonstrates more directed movement patterns. It tends to move persistently in a single direction (e.g., heading toward city exits), reducing random directional changes.\n",
    "- **Reduced Menu Misuse**: The agent still occasionally interacts with the game menu, but it now exits menus more quickly and does not get \"stuck\" inside menus nearly as often.\n",
    "- **Navigation Toward Objectives**: The agent shows a tendency to reach new areas and even approach entering Viridian City. This indicates some understanding (learned through reward feedback) that spatial exploration is valuable.\n",
    "- **State Awareness**: Unlike the random agent, the PPO agent occasionally stops or adjusts movement in response to blocked paths or obstacles, hinting at a learned association between certain states and actions.\n",
    "\n",
    "\n",
    "\n",
    "#### Critical Evaluation\n",
    "\n",
    "While the PPO agent's gameplay is still imperfect, it shows significant improvements in several areas compared to the RandomAgent baseline:\n",
    "\n",
    "| Aspect                  | RandomAgent                         | PPOAgent                                   |\n",
    "|--------------------------|-------------------------------------|-------------------------------------------|\n",
    "| Movement Directionality  | Random and chaotic                  | Mostly consistent, goal-oriented         |\n",
    "| Menu Interaction         | Frequent, disruptive               | Occasional, quickly exited               |\n",
    "| Progress Toward Goals    | Almost none                         | Partial — beginning to reach objectives  |\n",
    "| Reward Accumulation      | Minimal or none                     | Noticeably improved                      |\n",
    "\n",
    "However, despite clear evidence of learning, several limitations were observed:\n",
    "\n",
    "- **Training Time Constraints**: PPO, like most modern deep reinforcement learning algorithms, requires very large amounts of environment interaction to fully optimize its policy. Given the complexity and sparsity of the reward structure in Pokémon Red, training had to be curtailed before the agent could fully master the game mechanics. This model is after ~2,000 iterations of training with (167,772,160 total time steps, or ~20 hours)\n",
    "- **Partial Success Evidence**: Despite the limited training, during several experimental runs, the agent was observed to successfully defeat Brock and obtain the first badge. These successes demonstrate that the agent possesses the capacity to learn complex sequences** when given sufficient training time and feedback.\n",
    "- **Continued Learning Potential**: The agent's performance was still improving in the final training runs, suggesting that additional training would likely have further enhanced both reliability and gameplay sophistication.\n",
    "\n",
    "In summary, the PPO agent shows clear evidence of learning under challenging conditions, but also highlights the resource demands inherent to applying deep reinforcement learning to large, structured environments like Pokémon Red.\n",
    "\n",
    "<img src=\"../assets/gifs/ppo_agent.gif\" alt=\"PPO Agent\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535bf6c4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## A2C\n",
    "\n",
    "#### Agent Selection Rationale\n",
    "\n",
    "In addition to PPO, the **Advantage Actor-Critic (A2C)** algorithm was also implemented as a baseline reinforcement learning agent.  \n",
    "A2C is a synchronous variant of the Actor-Critic family of algorithms, combining **policy optimization** and **value function estimation** into a single framework.\n",
    "\n",
    "**Key reasons for exploring A2C include:**\n",
    "- **Simplicity**: A2C is algorithmically simpler than PPO and faster to implement and tune.\n",
    "- **Faster Updates**: A2C performs updates every `n_steps` without the need for complex clipping mechanisms like PPO, which can speed up early-stage learning.\n",
    "- **Strong Theoretical Foundations**: Actor-Critic methods directly optimize the policy while using a critic to reduce variance in policy gradient estimates, providing more stable updates than pure policy gradient methods.\n",
    "\n",
    "Although A2C is less stable and less robust compared to PPO in very large environments, it serves as a useful comparison point to evaluate how algorithmic complexity influences agent performance in Pokémon Red.\n",
    "\n",
    "\n",
    "\n",
    "#### Overview of A2C\n",
    "\n",
    "Advantage Actor-Critic (A2C) works by maintaining two neural networks:\n",
    "- **Actor**: Proposes actions based on the current policy.\n",
    "- **Critic**: Estimates the value of the current state to guide the actor's learning.\n",
    "\n",
    "The key idea behind A2C is to use the **advantage function** $A(s, a)$ to update the policy:\n",
    "\n",
    "$$A(s, a) = Q(s, a) - V(s)$$\n",
    "\n",
    "where:\n",
    "- $Q(s, a)$ is the estimated return for taking action $a$ in state $s$,\n",
    "- $V(s)$ is the estimated value of state $s $.\n",
    "\n",
    "This advantage reduces the variance of policy gradient updates, making learning more stable.\n",
    "\n",
    "\n",
    "\n",
    "## A2C Agent Implementation Details\n",
    "\n",
    "The A2C agent was implemented using **Stable-Baselines3**'s `A2C` algorithm.  \n",
    "The setup was intentionally kept similar to PPO for a fair comparison, with parallelized environments (`SubprocVecEnv`) used for sample efficiency.\n",
    "\n",
    "The model was configured as follows:\n",
    "\n",
    "```python\n",
    "model = A2C(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    n_steps=train_steps_batch,\n",
    "    gamma=0.997,\n",
    "    ent_coef=0.01,\n",
    "    tensorboard_log=sess_path\n",
    ")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=(ep_length) * num_cpu * 10_000,\n",
    "    callback=CallbackList(callbacks),\n",
    "    tb_log_name=\"poke_a2c\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### Key Hyperparameters\n",
    "\n",
    "The key hyperparameters used for training the A2C agent were:\n",
    "\n",
    "- **Policy**: `MultiInputPolicy`\n",
    "- **Batch Size**: Determined internally by `n_steps` (A2C does not use a separate batch size hyperparameter like PPO).\n",
    "- **Discount Factor (gamma)**: 0.997\n",
    "- **Entropy Coefficient**: 0.01 (to promote exploration)\n",
    "- **Number of Steps per Update (`n_steps`)**: Defined via `train_steps_batch`\n",
    "\n",
    "Similar to PPO, training time constraints limited the extent of optimization, as large amounts of environment interaction are necessary for effective learning in Pokémon Red.  The A2C agent was trained for a similar amount of time as the PPO agent, but had less training steps as it was slower and more memory intensive.\n",
    "\n",
    "\n",
    "\n",
    "## Gameplay Behavior and Observations\n",
    "\n",
    "The A2C agent displayed some improvements over the RandomAgent baseline, but way less polished behavior compared to the PPO agent:\n",
    "\n",
    "- **Directed Movement**: The agent generally moves in a similar chaotic fashion as the random agent. Indicating the agent still needs to train.\n",
    "- **Menu Interaction**: Similar to the random agent, the A2C agent spends a lot of unnecessary time in the menu moving around.\n",
    "- **Exploration Patterns**: The agent explores the map but appears less systematic than the PPO agent, sometimes retracing steps unnecessarily.\n",
    "- **Progress Toward Objectives**: Reaches new areas occasionally but at a slower pace than PPO.\n",
    "\n",
    "\n",
    "\n",
    "## Critical Evaluation\n",
    "\n",
    "The A2C agent demonstrates an ability to learn environmental structure and take meaningful actions, but with more variability and less reliability compared to PPO:\n",
    "\n",
    "| Aspect                   | RandomAgent                        | A2CAgent                                   | PPOAgent                                  |\n",
    "|---------------------------|------------------------------------|-------------------------------------------|------------------------------------------|\n",
    "| Movement Directionality   | Random and chaotic                 | Random and chaotic   | Mostly consistent, goal-oriented         |\n",
    "| Menu Interaction          | Frequent, disruptive              | Frequent, disruptive           | Rare and quickly exited                  |\n",
    "| Progress Toward Goals     | Almost none                        | Partial — slow but observable             | Partial — faster and more consistent     |\n",
    "| Reward Accumulation       | Minimal or none                    | Minimal or none                      | Noticeably improved                      |\n",
    "\n",
    "**Additional observations:**\n",
    "- **Training Time Constraints**: Similar to PPO, the A2C agent was limited by the extensive training time required to fully master the environment.\n",
    "- **Signs of Continued Learning**: The agent showed ongoing improvements during training, but no consistent achievement of major milestones (such as defeating Brock) within the recorded training sessions.\n",
    "\n",
    "Overall, the A2C agent represents a clear improvement over pure random behavior, but its performance lags slightly behind PPO, reflecting the known limitations of A2C in highly complex, sparse-reward environments.\n",
    "\n",
    "<img src=\"../assets/gifs/a2c_agent.gif\" alt=\"A2C Agent\" width=\"300\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c46500",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Results\n",
    "\n",
    "# Final Results Table\n",
    "\n",
    "| Agent            | Performance Summary                                                                                      | Training Notes                                              | Training Time | Total Training Steps | Gameplay Example                                      |\n",
    "|------------------|----------------------------------------------------------------------------------------------------------|-------------------------------------------------------------|---------------|----------------------|------------------------------------------------------|\n",
    "| **Random Agent** | - Purely random movement<br>- Frequent accidental menu openings<br>- No meaningful exploration or progress | - No training required; purely random baseline             | N/A           | N/A                  | ![Random Agent](../assets/gifs/Random_agent.gif)      |\n",
    "| **A2C Agent**     | - Partial goal-directed movement<br>- Occasional menu errors<br>- Some map exploration, but inefficient   | - Requires long training sessions<br>- Sensitive to memory constraints | 20 hours   | 84,541,440          | ![A2C Agent](../assets/gifs/a2c_agent.gif)            |\n",
    "| **PPO Agent**     | - Consistent and purposeful movement<br>- Minimal menu errors<br>- Strong exploration and level progress  | - High training time needed<br>- Most stable and efficient to optimize | 20 hours   |      167,772,160      | ![PPO Agent](../assets/gifs/ppo_agent.gif)            |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e729c5",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "\n",
    "The current set of experiments demonstrated that PPO consistently outperformed both the A2C and Random agents, showing strong goal-directed behavior, efficient exploration, and meaningful progress through the game environment. However, despite these promising early results, the agent’s training remains incomplete.\n",
    "\n",
    "Moving forward, I plan to continue training the PPO agent to achieve even more stable and optimized policies. One key area of improvement involves expanding the reward structure. Through additional research into the game's memory locations, I intend to introduce new reward signals that scale with the rarity and difficulty of Pokémon encountered, as well as the complexity of in-game events triggered. This refinement would provide the agent with a more nuanced understanding of game dynamics beyond simply exploration and badge collection.\n",
    "\n",
    "Additionally, I plan to implement a dynamic objective system where the agent is provided a list of high-level goals (e.g., \"reach Pewter City Gym\" or \"capture a rare Pokémon\"). The reward would then scale based on the agent’s distance from its current objective, encouraging more strategic, directed behavior instead of purely reactive exploration.\n",
    "\n",
    "These enhancements are intended to create a richer, denser reward landscape, improving both the sample efficiency and long-term learning potential of the PPO agent. Over time, I aim for the agent not only to reach early-game milestones but to progressively achieve higher-order objectives in Pokémon Red in a more human-like, goal-driven manner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe8f96bb",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "References\n",
    "Game Freak. (1996). Pokémon Red Version [Video game]. Nintendo.\n",
    "\n",
    "Nintendo. (1998). Pokémon Red Version Instruction Manual. Nintendo of America Inc. Retrieved from https://pokemon-project.com/juegos/manual/manual-GB-Pokemon-Rojo-Azul-EN.pdf\n",
    "\n",
    "Sutton, Richard S., & Barto, Andrew G. (2018). Reinforcement Learning: An Introduction (2nd ed.). MIT Press. Retrieved from http://incompleteideas.net/book/the-book-2nd.html\n",
    "\n",
    "Whiddy, Patrick. (2024). PokemonRedExperiments: A Gymnasium environment for Pokémon Red. GitHub [Pokemon Red env]. Retrieved from https://github.com/PWhiddy/PokemonRedExperiments\n",
    "\n",
    "OpenAI. (2016). OpenAI Gym: A toolkit for developing and comparing reinforcement learning algorithms. GitHub. Retrieved from https://github.com/openai/gym\n",
    "\n",
    "Brockman, Greg, Cheung, Vicki, Pettersson, Ludwig, Schneider, Jonas, Schulman, John, Tang, Jie, & Zaremba, Wojciech. (2016). OpenAI Gym [Software]. GitHub. Retrieved from https://github.com/openai/gym\n",
    "\n",
    "Danielsen, Baekgaard, Niklasson, Jakob, & others. (2018–2024). PyBoy: Game Boy Emulator for Reinforcement Learning Research [Software]. GitHub. Retrieved from https://github.com/Baekalfen/PyBoy\n",
    "\n",
    "Raffin, Antonin, Hill, Ashley, Gleave, Adam, Kanervisto, Anssi, Ernestus, Noah, & Dormann, Christian. (2021). Stable-Baselines3: Reliable Reinforcement Learning Implementations [Software]. GitHub. Retrieved from https://github.com/DLR-RM/stable-baselines3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
