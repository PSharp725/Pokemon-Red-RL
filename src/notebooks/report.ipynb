{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c69becf",
   "metadata": {},
   "source": [
    "# Attempting to Beat Brock in Pokémon Red using Reinforcement Learning\n",
    "\n",
    "## By: Patrick Sharp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42866464",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This project aims to explore Reinforcement Learning (RL) algorithms within the context of classic video game environments, specifically focusing on Pokémon Red. The project's goals are twofold:\n",
    "\n",
    "1. to implement and evaluate various RL algorithms in a custom Gymnasium environment based on Pokémon Red (thanks to [this repo](https://github.com/PWhiddy/PokemonRedExperiments/tree/master) by PWhiddy), and\n",
    "\n",
    "2. to investigate the critical balance between exploration and exploitation in RL agent performance.\n",
    "\n",
    "Unlike many traditional RL projects that focus on low-dimensional environments (e.g., CartPole, FrozenLake), this project attempts to tackle a partially observable, high-variance game world. This naturally introduces complexity in terms of state representation, reward sparsity, and policy generalization.\n",
    "\n",
    "The ultimate practical goal is to train agents capable of defeating Brock, the first gym leader, thereby obtaining the Boulder Badge — a milestone early in the game but nontrivial in terms of action selection, state abstraction, and long-term planning.\n",
    "\n",
    "### What is Pokémon Red?\n",
    "\n",
    "Pokémon Red (1996, Game Freak/Nintendo) is a role-playing video game (RPG) in which players control a protagonist navigating a fictional world, capturing and training creatures called Pokémon, and battling other trainers to earn badges and progress the storyline.\n",
    "\n",
    "From an RL perspective, Pokémon Red presents a sequential decision-making problem with attributes including:\n",
    "\n",
    "- Partial Observability: The agent cannot directly observe true world states (e.g., enemy Pokémon's hidden stats).\n",
    "\n",
    "- Long-Term Dependencies: Success depends not just on immediate actions but on strategies developed across long sequences (e.g., choosing to train a Pokémon early affects performance hours later).\n",
    "\n",
    "- Stochasticity: Many events (critical hits, enemy move choices) introduce randomness into outcomes.\n",
    "\n",
    "- Sparse Rewards: Winning a battle or earning a badge occurs only after potentially hundreds of intermediate steps without explicit reward signals.\n",
    "\n",
    "Thus, it provides a rich testbed beyond simplistic RL benchmarks.\n",
    "\n",
    "### Why Pokémon Red?\n",
    "\n",
    "Pokémon Red was selected due to personal nostalgia, as it was my first introduction to video games at the age of four, played on my yellow Gameboy Pocket. This nostalgic connection provides intrinsic motivation to dive deeper into the problem space.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## Setting up the environment \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### PyBoy\n",
    "\n",
    "[PyBoy](https://github.com/Baekalfen/PyBoy) is a Python-based emulator for the Nintendo Game Boy, designed to provide programmatic access to the emulation process through a clean API. It allows external scripts to read game memory, send controller inputs, and observe screen outputs — all crucial capabilities for integrating reinforcement learning agents with a game environment that was never originally designed for AI training.\n",
    "\n",
    "For this project, PyBoy acts as the critical bridge between the RL algorithms and Pokémon Red. It enables the custom Gymnasium environment to interface directly with the game's internal state, sending actions (e.g., pressing 'A', 'Start', navigating menus) and receiving observations (e.g., screen pixels, memory values) in a way that is compatible with modern RL pipelines. Without such programmatic control and visibility into game state, training agents in a complex environment like Pokémon Red would be effectively infeasible.\n",
    "\n",
    "Moreover, using PyBoy ensures deterministic, reproducible experiments — an essential property for debugging RL agents, evaluating exploration strategies, and properly measuring algorithmic performance.\n",
    "\n",
    "### Action Space\n",
    "\n",
    "The game environment takes these controls and creates the following action lists that can be used within the environment wrapper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8ea71f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"Using SDL2 binaries from pysdl2-dll*\")\n",
    "from pyboy.utils import WindowEvent\n",
    "\n",
    "valid_actions = [\n",
    "            WindowEvent.PRESS_ARROW_DOWN,\n",
    "            WindowEvent.PRESS_ARROW_LEFT,\n",
    "            WindowEvent.PRESS_ARROW_RIGHT,\n",
    "            WindowEvent.PRESS_ARROW_UP,\n",
    "            WindowEvent.PRESS_BUTTON_A,\n",
    "            WindowEvent.PRESS_BUTTON_B,\n",
    "            WindowEvent.PRESS_BUTTON_START,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953847c7",
   "metadata": {},
   "source": [
    "We omit `BUTTON_SELECT` and `NOOP` from the available actions, as choosing them would not meaningfully contribute to exploration and could hinder agent progress.\n",
    "\n",
    "![alt text](../assets/images/Pokemon_red_controls.png \"Controls\")\n",
    "\n",
    "Nintendo. (1996) [Pokémon Red Trainer's Guide](../../Pokemon-Red-User-Manual.pdf). Nintendo of America Inc. Retrieved from https://pokemon-project.com/juegos/manual/manual-GB-Pokemon-Rojo-Azul-EN.pdf\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Verifying the game file\n",
    "\n",
    "Before using Pokémon Red within the custom Gymnasium environment, it is critical to ensure that the game file (ROM) being used matches the expected version supported by the environment. To do this, we verify the integrity of the PokemonRed.gb file by calculating its SHA-1 checksum.\n",
    "\n",
    "Using the following command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5ee7d3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ea9bcae617fdf159b045185467ae58b2e4a48b9a  ../../PokemonRed.gb\n"
     ]
    }
   ],
   "source": [
    "# Check the hash of the ROM file\n",
    "!shasum ../../PokemonRed.gb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00529968",
   "metadata": {},
   "source": [
    "we compute the SHA-1 hash of the ROM file. The expected hash, according to PWhiddy's repository documentation, is: `ea9bcae617fdf159b045185467ae58b2e4a48b9a`.\n",
    "\n",
    "If the output of the command matches this expected value, it confirms that the ROM file is identical at the binary level to the one the custom Gymnasium environment was built and tested against."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba43613f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2cb3f5d9",
   "metadata": {},
   "source": [
    "## Approach\n",
    "\n",
    "\n",
    "### RL Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f76a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "from red_gym_env import RedGymEnv\n",
    "\n",
    "STATE_REWARDS = {\n",
    "    \"event\": True,\n",
    "    \"level\": False,\n",
    "    \"heal\": True,\n",
    "    \"op_lvl\": False,\n",
    "    \"dead\": False,\n",
    "    \"badge\": True,\n",
    "    \"explore\": True,\n",
    "    \"stuck\": True,\n",
    "}\n",
    "STATE_REWARD_WEIGHTS = {\n",
    "    \"event\": 4,\n",
    "    \"level\": 1,\n",
    "    \"heal\": 10,\n",
    "    \"op_lvl\": 0.2,\n",
    "    \"dead\": -0.1,\n",
    "    \"badge\": 10,\n",
    "    \"explore\": 0.1,\n",
    "    \"stuck\": -0.05,\n",
    "}\n",
    "REWARD_SCALE = 0.5\n",
    "EXPLORE_WEIGHT = 0.25\n",
    "EP_LENGTH = 2048 * 80\n",
    "NUM_CPU = 32  # Also sets the number of episodes per training iteration\n",
    "\n",
    "env_config = {\n",
    "    'headless': True,\n",
    "    'save_final_state': True,\n",
    "    'early_stop': False,\n",
    "    'action_freq': 24,\n",
    "    'init_state': './init.state',\n",
    "    'max_steps': EP_LENGTH,\n",
    "    'save_video': True,\n",
    "    'fast_video': False,\n",
    "    'session_path': sess_path,\n",
    "    'gb_path': './PokemonRed.gb',\n",
    "    'debug': False,\n",
    "    'reward_scale': REWARD_SCALE,\n",
    "    'explore_weight': EXPLORE_WEIGHT,\n",
    "    'print_rewards': True,\n",
    "}\n",
    "env = RedGymEnv(env_conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb7bb67",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Random Agent\n",
    "\n",
    "```Python\n",
    "import numpy as np\n",
    "from agents.base_agent import BaseAgent\n",
    "\n",
    "class RandomAgent(BaseAgent):\n",
    "    def __init__(self, action_space):\n",
    "        super().__init__(action_space)\n",
    "\n",
    "    def select_action(self, observation):\n",
    "        return self.action_space.sample()\n",
    "\n",
    "env = RedGymEnv(env_config)\n",
    "agent = RandomAgent(env.action_space)\n",
    "\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "\n",
    " while not done:\n",
    "    action = agent.select_action(obs)\n",
    "    obs, reward, _, done, _ = env.step(action)\n",
    "    env.save_and_print_info(done, obs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ef3907",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "\n",
    "### PPO\n",
    "\n",
    "```Python\n",
    "def make_env(rank, env_conf, seed=0):\n",
    "    \"\"\"\n",
    "    Utility function for multiprocessed env.\n",
    "    :param env_id: (str) the environment ID\n",
    "    :param num_env: (int) the number of environments you wish to have in subprocesses\n",
    "    :param seed: (int) the initial seed for RNG\n",
    "    :param rank: (int) index of the subprocess\n",
    "    \"\"\"\n",
    "    def _init():\n",
    "        env = RedGymEnv(env_conf)\n",
    "        env.reset(seed=(seed + rank))\n",
    "        return env\n",
    "    set_random_seed(seed)\n",
    "    return _init\n",
    "\n",
    "\n",
    "num_cpu = NUM_CPU \n",
    "env = SubprocVecEnv([make_env(i, env_config) for i in range(NUM_CPU)])\n",
    "\n",
    "checkpoint_callback = CheckpointCallback(\n",
    "    save_freq=ep_length//2,\n",
    "    save_path=sess_path,\n",
    "    name_prefix=\"poke\"\n",
    ")   \n",
    "callbacks = [checkpoint_callback, TensorboardCallback(sess_path)]\n",
    "\n",
    "model = PPO(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    n_steps=train_steps_batch,\n",
    "    batch_size=512,\n",
    "    n_epochs=1,\n",
    "    gamma=0.997,\n",
    "    ent_coef=0.01,\n",
    "    tensorboard_log=sess_path\n",
    ")\n",
    "model.learn(\n",
    "    total_timesteps=(ep_length) * num_cpu * 10_000,  # Attempt to run 10,000 iterations\n",
    "    callback=CallbackList(callbacks),\n",
    "    tb_log_name=\"poke_ppo\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289b4eca",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### A2C\n",
    "\n",
    "\n",
    "```Python\n",
    "model = A2C(\n",
    "    \"MultiInputPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    n_steps=train_steps_batch,\n",
    "    gamma=0.997,\n",
    "    ent_coef=0.01,\n",
    "    tensorboard_log=sess_path\n",
    ")\n",
    "\n",
    "model.learn(\n",
    "    total_timesteps=(ep_length) * num_cpu * 10000,\n",
    "    callback=CallbackList(callbacks),\n",
    "    tb_log_name=\"poke_a2c\"\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec27728",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
